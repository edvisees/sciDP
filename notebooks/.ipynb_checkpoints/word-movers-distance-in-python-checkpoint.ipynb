{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word mover's distance classification in Python\n",
    "\n",
    "*A guide to scikit-learn compatible nearest neighbors classification using the recently introduced word mover's distance (WMD). *\n",
    "Joint post with the awesome [Matt Kusner](http://matthewkusner.com)!\n",
    "\n",
    "[Source of this Jupyter notebook.](http://nbviewer.jupyter.org/github/vene/vene.github.io/blob/pelican/content/blog/word-movers-distance-in-python.ipynb)\n",
    "\n",
    "In document classification and other natural language processing applications, having a good measure of the similarity of two texts can be a valuable building block.   Ideally, such a measure would capture semantic information.  Cosine similarity on bag-of-words vectors is known to do well in practice, but it inherently cannot capture when documents say the same thing in completely different words.\n",
    "\n",
    "Take, for example, two headlines:\n",
    "\n",
    " * *Obama speaks to the media in Illinois*\n",
    " * *The President greets the press in Chicago*\n",
    "\n",
    "These have no content words in common, so according to most bag of words--based metrics, their distance would be maximal.  (For such applications, you probably don't want to count stopwords such as *the* and *in*, which don't truly signal semantic similarity.)\n",
    "\n",
    "One way out of this conundrum is the word mover's distance (WMD), introduced in \n",
    "[*From Word Embeddings To Document Distances*](http://mkusner.github.io/publications/WMD.pdf),\n",
    "(Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger, ICML 2015).\n",
    "WMD adapts the [earth mover's distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance) to the space of documents: the distance between two texts is given by the total amount of \"mass\" needed to move the words from one side into the other, multiplied by the distance the words need to move. So, starting from a measure of the distance between different words, we can get a principled document-level distance. Here is a visualisation of the idea, from the ICML slides:\n",
    "\n",
    "![WMD example from Matt's slides](https://vene.ro/images/wmd-obama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare some word embeddings\n",
    "\n",
    "The key ingredient in WMD is a good distance measure between words.  Dense representations of words, also known by the trendier name \"word embeddings\" (because \"distributed word representations\" didn't stick), do the trick here.  We could train the embeddings ourselves, but for meaningful results we would need tons of documents, and that might take a while. So let's just use the ones from the [`word2vec`](https://code.google.com/p/word2vec/) team. [(download link)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s6 (,hypothesis|implication) [X] : In support of distinct peroxisomal binding sites for Pex7p , the Pex7p/Pex13p and Pex7p/ Pex14p complexes can form independently. Genetic evidence for the interaction of Pex7p and Pex13p is provided by the observation that overexpression of Pex13p suppresses a loss of function mutant of Pex7p .\n",
      "s8 (,implication|implication) [0] : NH2-terminal regions of Pex13p are required for its interaction with the PTS2-receptor while the COOH-terminal SH3 domain alone is sufficient to mediate its interaction with the PTS1-receptor .\n",
      "s9 (,result|implication) [0] : Reinvestigation of the topology revealed both termini of Pex13p to be oriented towards the cytosol .\n",
      "s10 (,result|implication) [0] : We also found Pex13p to be required for peroxisomal association of Pex14p , yet the SH3 domain of Pex13p may not provide the only binding site for Pex14p at the peroxisomal membrane .\n",
      "s18 (,result|result) [0] : pex7Delta cells exhibit the reverse phenotype ( for review see exLink ) .\n",
      "s19 (,implication) [0] : The intracellular localization of both targeting signal receptors is still a matter of debate .\n",
      "s20 (,result|result) [0] : A predominantly cytosolic , membrane-bound , and even intraperoxisomal localization have been reported for both receptors ( for review see exLink ) .\n",
      "s23 (,result|implication) [0] : There is no experimental evidence for this model , but it is consistent with the observation that peroxisomes are able to import both folded and oligomeric proteins ( for review see exLink ) .\n",
      "s30 (,implication|implication) [0] : Together , these data suggest that the two import pathways are not independent but overlapping , with Pex14p as the point of convergence of the pathways at the peroxisomal membrane ( exLink ) .\n",
      "s33 (,result|method) [X] : In addition , overexpression of Pex13p suppresses the protein import defect caused by HA-tagged , functionally compromised Pex7p , further suggesting an interaction between the two proteins by genetic means .\n",
      "s35 (,result|result) [0] : Reinvestigation of the membrane topology of Pex13p revealed that both termini of the protein are exposed to the cytosol .\n",
      "s36 (,implication) [0] : Pex13p was also required for Pex14p localization at the peroxisomal membrane .\n",
      "s37 (,result) [0] : However , the peroxisomal targeting of Pex14p did not require interaction with the SH3 domain of Pex13p .\n",
      "s133 (Results,result|hypothesis) [0] : It has been reported that the import receptors Pex5p and Pex7p interact with each other in the yeast two-hybrid system , which opened the possibility that both proteins may form a heteromeric cytosolic signal recognition complex ( exLink ) .\n",
      "s134 (Results,result|hypothesis) [0] : However , the yeast two-hybrid system does not necessarily distinguish between a direct and indirect binding of two S. cerevisiae proteins , as endogenous proteins may contribute to the observed interaction .\n",
      "s135 (Results,hypothesis|result) [X] : As Pex14p can bind both import receptors , we investigated whether the Pex5p/Pex7p interaction is still observed in a yeast two-hybrid reporter strain deleted for the genomic copy of PEX14 ( exLink ) .\n",
      "s143 (Results,method|result) [0] : Pex14p and Pex13p , but not Fbp1p , pelleted , indicating the complete sedimentation of cytosol-free peroxisomal membranes ( Fig. 2 ) .\n",
      "s144 (Results,result|result) [0] : As reported previously ( exLink ) , mycPex7p was predominantly found in the soluble fraction in wild-type cells , while a low but significant amount was detected in the membrane fraction .\n",
      "s145 (Results,result|result) [0] : A decrease of mycPex7p in the pellet fraction of pex14Delta cells ( Fig. 2 ) suggests that the majority of sedimentable Pex7p associates with membranes in a Pex14p-dependent manner .\n",
      "s146 (Results,result|implication) [X] : However , in pex14Delta cells a significant amount of mycPex7p was detected in the membrane pellet fraction ( Fig. 2 ) , indicating that next to Pex14p additional binding factors for Pex7p exist at the peroxisomal membrane .\n",
      "s147 (Results,result) [0] : Coimmunoprecipitation of Pex13p and Pex7p in the Absence of Pex14p and Pex5p\n",
      "s149 (Results,result|result) [0] : As reported previously , we found Pex5p , Pex13p , Pex14p , and Pex17p associated with mycPex7p when the receptor was precipitated from wild-type or complemented pex7Delta cells ( exLink ; exLink ) .\n",
      "s150 (Results,result) [0] : Comparison of the constituents of the precipitates revealed five interesting observations .\n",
      "s151 (Results,result|implication) [0] : First , in pex14Delta and pex5Delta/pex14Delta strains , Pex13p still coimmunoprecipitated with mycPex7p ( Fig. 3 ) , suggesting that Pex13p associates directly or indirectly with Pex7p .\n",
      "s152 (Results,result|implication) [0] : Moreover , this result indicated that neither Pex14p nor Pex5p is required for the formation of this subcomplex of Pex13p and Pex7p .\n",
      "s153 (Results,result|result) [X] : Second , the amount of Pex5p in the precipitate from pex14Delta cells was drastically reduced , while the amount in Pex13p remained essentially unchanged ( Fig. 3 , lane pex14Delta ) .\n",
      "s154 (Results,implication|implication) [X] : This result supports the notion that the amount of Pex5p bound to Pex13p does not determine the stoichiometry of the Pex13p-Pex7p subcomplex .\n",
      "s155 (Results,implication|implication) [0] : However , it also suggests that Pex13p may not bind both import receptors equally at the same time .\n",
      "s156 (Results,result) [0] : Third , Pex13p , Pex14p , and Pex5p still coimmunoprecipitated with Pex7p in pex17Delta cells ( Fig. 3 , lane pex17Delta ) .\n",
      "s157 (Results,fact|implication) [0] : Obviously , Pex7p is associated with components of the peroxisomal translocation machinery in the absence of Pex17p , suggesting that the presence of Pex17p is not a prerequisite for docking of Pex7p to the peroxisomal membrane .\n",
      "s158 (Results,result|implication) [0] : Fourth , the lack of Pex17p in the coimmunoprecipitate from pex14Delta cells ( Fig. 3 , lane pex14Delta ) , suggests that Pex14p is required for the association of Pex17p with the complex , and is consistent with the assumption that Pex17p binding to the complex may be via Pex14p .\n",
      "s159 (Results,implication|result) [0] : However , this observation must be interpreted with care since the pex14Delta cells contain much less immunologically detectable Pex17p ( exLink ) .\n",
      "s160 (Results,result|result) [X] : Finally , the amount of Fox3p that coimmunoprecipitates with Pex7p drastically increases in mutants with an import defect for PTS2 proteins ( pex17Delta , pex13Delta , pex14Delta , and pex5Delta/pex14Delta ) relative to the strains unaffected in this pathway ( wild-type and pex5Delta ) .\n",
      "s161 (Results,result|result) [X] : Since the total amount of both proteins is similar in all strains ( Fig. 3 B ) , it seems unlikely that the observed Pex7p/Fox3p complex has formed in vitro after cell disruption .\n",
      "s162 (Results,hypothesis|implication) [0] : A simple explanation for this may be that the high cytosolic concentration of thiolase in the import mutants results in greater occupation of the PTS2 receptor .\n",
      "s164 (Results,result|result) [0] : These proteins were not detected in any of the samples , indicating the specificity of the observed interactions ( data not shown ) .\n",
      "s166 (Results,result|goal) [0] : The observed in vivo association of Pex7p with Pex13p in cells lacking Pex14p and Pex5p encouraged us to analyze the interaction of these proteins in more detail .\n",
      "s169 (Results,result|result) [0] : The results shown in Fig. 4 A reveal that the full length Pex13p is indeed able to interact with the PTS2-receptor Pex7p .\n",
      "s170 (Results,result|result) [0] : The controls included show that coexpression of either of the fusion proteins alone did not support transcription activation of the reporter genes .\n",
      "s173 (Results,result|result) [X] : Because Pex13pE320K lost the ability to interact with Pex14p in the yeast two-hybrid system ( Fig. 4 B , see also Fig. 8 ) , this experiment was expected to monitor the Pex13p/Pex7p interaction upon simultaneous elimination of the Pex14p and Pex5p influence .\n",
      "s174 (Results,result|result) [0] : As shown in Fig. 4 , these two-hybrid analyses did not reveal an influence of Pex5p or Pex14p on the Pex13p/ Pex7p interaction .\n",
      "s175 (Results,result|result) [X] : No difference was observed independent of whether the Pex7p/Pex13p interaction was analyzed in wild-type , pex5Delta , or pex14Delta strains ( Fig. 4 A ) , or for the Pex7p/Pex13pE320K interaction in pex5Delta cells ( Fig. 4 B ) .\n",
      "s176 (Results,implication|implication) [0] : These results indicate that neither Pex14p nor Pex5p is required for the in vivo interaction of Pex7p with Pex13p , and therefore are in agreement with results obtained in the coimmunoprecipitation experiment ( Fig. 3 ) .\n",
      "s177 (Results,result|implication) [0] : The two-hybrid interaction of the complete Pex13p with Pex14p is only detected by histidine prototrophy ( Fig. 4 B ) , indicating that regions NH2-terminal of the SH3 domain of Pex13p may weaken the interaction of these proteins in the two-hybrid system .\n",
      "s179 (Results,fact|result) [0] : Mutant cells lacking Pex7p are characterized by their inability to grow on oleic acid as the sole carbon source ( Fig. 5 A ) and by mislocalization of peroxisomal thiolase to the cytosol ( exLink ; exLink ) .\n",
      "s180 (Results,result) [X] : Expression of a COOH-terminally HA-tagged Pex7p from the low copy plasmid pRSPEX7-HA3 leads only to a partial complementation of the pex7Delta mutant phenotype ( exLink ) .\n",
      "s181 (Results,implication|result) [0] : This is indicated by the inability of the transformants to grow on oleic acid plates ( Fig. 5 A ) and a reduced ability to import Fox3p ( thiolase ) into peroxisomes .\n",
      "s182 (Results,result) [0] : The latter is evident by the pronounced cytosolic mislocalization of this protein ( Fig. 5 B , panel d ) .\n",
      "s183 (Results,result|goal) [X] : This mutant phenotype of pex7Delta [ pRSPEX7-HA3 ] was employed to investigate whether overexpression of Pex7p-binding partners may suppress a defect in Pex7p function .\n",
      "s185 (Results,result|result) [X] : As judged by their growth characteristics on oleic acid medium ( Fig. 5 A ) and by the fluorescence pattern for thiolase ( Fig. 5 B ) , overexpression of PEX13 , but not PEX14 , rescued the mutant phenotype caused by the defective Pex7p-HA .\n",
      "s186 (Results,result|implication) [X] : Even though the suppression was not as efficient as complementation with the wild-type PEX7 , this observation demonstrates that Pex13p can suppress the mutant phenotype of pex7Delta [ pRSPEX7-HA3 ] , providing genetic evidence for an interaction between Pex7p and Pex13p .\n",
      "s191 (Results,result|result) [0] : The tag has been shown previously not to affect the function of Pex13p ( exLink ) .\n",
      "s193 (Results,result|result) [X] : As judged by immunoblot analysis , both the NH2-terminal myc-tag as well as the SH3 domain of Pex13p were rapidly degraded by the protease ( Fig. 6 ) .\n",
      "s194 (Results,result) [0] : Intraperoxisomal thiolase remained stable under these conditions and was only degraded in the presence of detergents ( data not shown ) .\n",
      "s195 (Results,implication|implication) [0] : From this data , we conclude that both the NH2 terminus and the COOH-terminal SH3 domain are exposed to the cytosol .\n",
      "s196 (Results,implication) [0] : This result also implicates the presence of an even number of transmembrane spans within Pex13p .\n",
      "s203 (Results,method|implication) [0] : This observation suggests that Pex17p is not required for the targeting of Pex14p to the peroxisomal membrane .\n",
      "s204 (Results,implication) [0] : In contrast , no congruent fluorescence patterns were observed in pex13Delta cells .\n",
      "s205 (Results,result|fact) [0] : Since the HA-tagged Pex11p is known to be targeted to peroxisomal membrane ghosts in pex13Delta cells ( exLink ) , the lack of congruence suggests that the majority of Pex14p is mislocalized .\n",
      "s206 (Results,implication|goal) [0] : To confirm this result by independent means , we performed a flotation of wild-type , pex13Delta , and pex17Delta homogenates in sucrose gradients ( Fig. 7 B ) .\n",
      "s209 (Results,fact|result) [0] : However , Pex14p was not detected in these fractions , but was found to cosegregate with mitochondrial fumarase .\n",
      "s210 (Results,fact|implication) [0] : These data suggest that the peroxisomal membrane ghosts in pex13Delta cells lack Pex14p .\n",
      "s211 (Results,implication) [0] : Thus , the presence of Pex13p is a prerequisite for peroxisomal membrane association of Pex14p .\n",
      "s212 (Results,implication|hypothesis) [0] : Pex13p could be involved in targeting , or it could be required for binding or retention of Pex14p at the peroxisome .\n",
      "s217 (Results,result) [0] : Remarkably , the mutated Pex14pAXXA still complemented the peroxisome biogenesis defect of pex14Delta cells ( data not shown ) .\n",
      "s219 (Results,result|result) [0] : This mutation has been reported to result in the inactivation of Pex13p function ( exLink ) .\n",
      "s220 (Results,result|result) [0] : As shown in Fig. 8 , the mutated Pex14pAXXA had lost the ability to bind Pex13p in the yeast two-hybrid system while binding to Pex5p , Pex7p , and oligomerization of the protein was unchanged .\n",
      "s221 (Results,result) [0] : Also the E320K mutation of Pex13p abolished the two-hybrid interaction of the SH3 domain of Pex13p with Pex14p ( Fig. 8 ) .\n",
      "s222 (Results,implication|implication) [0] : These results suggest that strong interactions between Pex14p and the SH3 domain of Pex13p are dependent on the PXXP motif within Pex14p , as well as on the RT loop of the SH3 domain of Pex13p .\n",
      "s223 (Results,method|result) [0] : Next , we analyzed the Pex14pAXXA ( Fig. 9 A ) association with peroxisomal membrane ghosts of pex14Delta/pex17Delta double mutants which were predicted to contain peroxisomal membrane ghosts even upon complementation of the pex14Delta mutation .\n",
      "s226 (Results,result) [0] : Colocalization was observed for HA-Pex11p and Pex14pAXXA in pex14Delta cells , as well as for HA-Pex11p and Pex14p in pex13Delta cells expressing Pex13pE320K , indicative of peroxisomal membrane association of these proteins ( Fig. 9 A ) .\n",
      "s227 (Results,result|result) [0] : These results were corroborated by flotation analysis which revealed that Pex14pAXXA was associated with the fraction containing the peroxisomal membrane ghosts of pex14Delta/pex17Delta , as were Pex14p in pex13Delta/pex17Delta cells expressing Pex13pE320K ( Fig. 9 B ) .\n",
      "s228 (Results,implication|implication) [0] : These observations suggest that Pex14p is associated with peroxisomes and peroxisomal membrane ghosts independent of interaction between the proline-rich motif of Pex14p and the RT loop in the SH3 domain of Pex13p .\n",
      "s229 (Results,result|implication) [0] : Interestingly , the fractionation of pex13Delta/ pex17Delta [ PEX13E320K ] shows that , although the RT loop of the SH3 domain of Pex13p is not absolutely required for the targeting of Pex14p to the membrane of peroxisomal ghosts , it appears to enhance or stabilize the targeting , as only Pex14p trails through the gradients of this mutant strain ( Fig. 9 B ) .\n",
      "s230 (Discussion,implication) [0] : Discussion\n",
      "s231 (Discussion,implication|implication) [0] : The peroxisomal membrane protein Pex14p has been reported to bind both the PTS1 and the PTS2 receptor , which led exLink to the conclusion that Pex14p may represent the point of convergence of the PTS1 - and PTS2-dependent protein import pathways at the peroxisomal membrane .\n",
      "s234 (Discussion,implication|implication) [0] : Pex13p is also shown to be required for the peroxisomal association of Pex14p ; however , evidence is provided that the SH3 domain of Pex13p may not represent the only binding site for Pex14p at the peroxisomal membrane .\n",
      "s236 (Discussion,result|fact) [0] : The SH3 domain of Pex13p has been reported to interact with the PTS1 receptor Pex5p and with Pex14p ( exLink ; exLink ; exLink ; exLink ; exLink ; Fig. 8 ) .\n",
      "s237 (Discussion,fact|result) [X] : A mutation in the RT loop of the SH3 domain of Pex13p , as well as a mutation of a putative class II SH3 ligand motif of Pex14p abolished the two-hybrid interaction of both proteins ( Fig. 8 ) , supporting the notion of a typical SH3 domain-ligand interaction between Pex13p and Pex14p .\n",
      "s238 (Discussion,result|result) [0] : Interestingly , although the E320K mutation of the RT loop of the SH3 domain of Pex13p abolishes its two-hybrid interaction with Pex14p , the mutated SH3 domain still interacts with Pex5p ( Fig. 8 B ) .\n",
      "s239 (Discussion,implication|implication) [0] : Accordingly , we conclude that there are distinct binding sites for both Pex5p and Pex14p within this domain or adjacent regions contained within the construct used for the assay .\n",
      "s240 (Discussion,result) [X] : Remarkably , neither the E320K mutation of the SH3 domain of Pex13p nor the mutation of the proline-rich motif of Pex14p prevented the peroxisomal localization of Pex14p ( Fig. 9 ) .\n",
      "s241 (Discussion,implication|implication) [0] : This observation suggests that the binding of Pex14p to the SH3 domain of Pex13p is not absolutely required for the targeting and binding of Pex14p to peroxisomes .\n",
      "s242 (Discussion,result) [0] : Why then does the absence of Pex13p lead to the mistargeting of Pex14p ( Fig. 7 ) ?\n",
      "s247 (Discussion,hypothesis|implication) [0] : It is true that Pex17p is another binding partner of Pex14p , but our data suggest that Pex17p is not required for association of the Pex13p/ Pex14p/Pex5p/Pex7p complex , as all these components can efficiently coprecipitate in the absence of Pex17p ( Fig. 3 ) .\n",
      "s248 (Discussion,result|result) [0] : Moreover , we found no Pex17p in a precipitate from pex14Delta cells that still contains Pex13p and Pex7p ( Fig. 3 ) , leading to two conclusions .\n",
      "s252 (Discussion,result|implication) [X] : The amount of Pex7p in the membrane sediment of pex14Delta cells is significantly lower than in wild-type or pex13Delta cells ( Fig. 2 ) , suggesting that Pex14p may contribute to the majority of the total binding capacity of the peroxisomal membrane for the PTS2 receptor .\n",
      "s253 (Discussion,result) [X] : However , a significant amount of Pex7p was sedimented in the absence of Pex14p ( Fig. 2 , lane pex14Delta ) .\n",
      "s254 (Discussion,result|implication) [X] : Interestingly , in cells lacking both Pex13p and Pex14p , no Pex7p was found in the membrane pellet , which suggests that Pex13p contributed to the remaining Pex7p associated with peroxisomal membranes of pex14Delta cells ( data not shown ) .\n",
      "s255 (Discussion,implication|result) [0] : This result , however , has to be interpreted with care since the double deletion of PEX13 and PEX14 did result in a significant decrease in immunologically detectable Pex7p ( Girzalsky , W. , and R. Erdmann , unpublished observations ) .\n",
      "s256 (Discussion,result|result) [0] : The observations that Pex13p and Pex7p interact in the two-hybrid system and can be efficiently coimmunoprecipitated indicate that the proteins interact in vivo ( Figs. 3 and 4 ) .\n",
      "s257 (Discussion,implication|result) [X] : Whether Pex13p directly binds Pex7p remains to be shown .\n",
      "s258 (Discussion,method|result) [0] : Attempts to demonstrate direct binding of the proteins by coimmunoprecipitation of in vitro translated proteins were unsuccessful ( data not shown ) .\n",
      "s260 (Discussion,implication|implication) [0] : However , two observations indicate that the hypothetical bridging protein is not one of the known binding partners for Pex13p .\n",
      "s261 (Discussion,result|result) [X] : First , the Pex7p/Pex13p interaction is also observed in the absence of these proteins ( Figs. 3 and 4 ) , and second , the COOH-terminal SH3 domain alone is sufficient for the Pex13p/ Pex14p and Pex13p/Pex5p two-hybrid interaction , but not for the interaction of Pex13p with Pex7p ( exLink ) .\n",
      "s262 (Discussion,implication) [X] : A direct interaction of Pex13p and Pex7p is further suggested by the genetic suppression of the defect caused by a functionally compromised HA-tagged Pex7p by overexpression of Pex13p ( Fig. 5 ) .\n",
      "s263 (Discussion,implication|result) [0] : As discussed above , a Pex5p/Pex7p two-hybrid interaction is not observed in pex14Delta ( Fig. 1 ) .\n",
      "s264 (Discussion,implication|result) [X] : At first , this observation seems rather surprising , since both Pex5p and Pex7p independently interact with Pex13p in the two-hybrid system ( Fig. 4 ) .\n",
      "s267 (Discussion,implication) [X] : In support of this assumption , the amount of Pex5p coimmunoprecipitating with Pex7p in the absence of Pex14p is extremely reduced , despite the presence of significant amounts of Pex13p ( Fig. 3 , lane pex14Delta ) .\n",
      "s268 (Discussion,implication|result) [0] : Perhaps Pex13p does not usually associate simultaneously with both of the import receptors , or association is transient .\n",
      "s271 (Discussion,result|result) [0] : One group has reported that the protein is exclusively localized in the peroxisomal lumen ( exLink , exLink ) , whereas others found the protein to be predominantly localized in the cytosol with a small amount associated with the peroxisomal membrane ( exLink ; exLink ) .\n",
      "s272 (Discussion,implication|hypothesis) [0] : Because the SH3 domain alone does not mediate the interaction with Pex7p , we suggest that regions NH2-terminal of the SH3 domain may be required for the interaction or contribute to the correct conformation of the binding site .\n",
      "s273 (Discussion,result|result) [0] : Previously , the COOH-terminal SH3 domain has been reported to face the cytosol ( exLink ; exLink ) , and we found that both the NH2 terminus and the COOH terminus of Pex13p are exposed to the cytosol ( Fig. 6 ) , suggesting that the protein traverses the membrane with an even number of membrane spans .\n",
      "s274 (Discussion,implication|hypothesis) [0] : In this respect , it is interesting to note that two regions which would fulfill the requirement for alpha-helical transmembrane segments are present in Pex13p ( exLink ) .\n",
      "s275 (Discussion,fact|implication) [0] : The interaction of Pex13p with Pex7p has far reaching implications for our understanding of protein import into the peroxisomal matrix .\n",
      "\n",
      "105/279\n",
      "25/39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "inFile = '/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/intact/scidt_fries_bioc_tsv4/10087260.tsv'\n",
    "#/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/corpora/pathwayLogic/scidt_bioc_sentences_tsv/11777939.tsv'\n",
    "tsv = pd.read_csv(inFile, sep='\\t')\n",
    "sentences = []\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "regex1 = re.compile(r\"[\\(\\)\\{\\}\\[\\]\\;\\.\\'\\\"\\,\\/\\_\\*]\", re.IGNORECASE)\n",
    "regex2 = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "\n",
    "allHits = 0\n",
    "hits = 0\n",
    "j = 0\n",
    "for i, row in tsv.iterrows():\n",
    "    sid = row['SentenceId']\n",
    "    codeStr = row['Codes']\n",
    "    paragraph = row['Paragraph']\n",
    "    text = row['Sentence Text']\n",
    "    heading = row['Headings']\n",
    "    floatingBox = row['FloatingBox?']\n",
    "    discourse = row['Discourse Type']\n",
    "    reachData = row['friesEventsTypes']\n",
    "    \n",
    "    j += 1\n",
    "    if(reachData == reachData):\n",
    "        allHits += 1\n",
    "\n",
    "    if (heading != heading):\n",
    "        heading = \"\"\n",
    "\n",
    "    if (floatingBox):\n",
    "        continue\n",
    "\n",
    "    if (('implication' not in discourse) and\n",
    "            'result' not in discourse):\n",
    "        continue\n",
    "\n",
    "    if ('methods' in heading.lower()):\n",
    "        continue\n",
    "       \n",
    "    r = 'X'\n",
    "    if(reachData != reachData):\n",
    "        r = '0'\n",
    "        \n",
    "    if(reachData == reachData):\n",
    "        hits += 1\n",
    "\n",
    "    print(sid + ' (' + heading + ',' + discourse + ') ' + '[' + r + '] : ' + text ) \n",
    "    \n",
    "    text = re.sub(regex1,\"\",text)\n",
    "    sent = regex2.split(text)\n",
    "    sent = [w for w in sent if w not in stopwords and len(w)>0]\n",
    "    sentences.append(sent)\n",
    "\n",
    "    if 'exLink' in codeStr:\n",
    "        continue\n",
    "\n",
    "    \n",
    "print\n",
    "print (str(len(sentences)) + '/' + str(j))\n",
    "print (str(hits) + '/' + str(allHits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        \"/Users/Gully/Documents/Projects/2_active/bigMech/work/2017-01-30-ldk_paper/embeddings_pubmed_files/PMC-w2v.bin\",\n",
    "        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(iter=1)  \n",
    "model.wv = wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences = 105\n",
      "\n",
      "........................................................................................................\n",
      ".......................................................................................................\n",
      "......................................................................................................\n",
      ".....................................................................................................\n",
      "....................................................................................................\n",
      "...................................................................................................\n",
      "..................................................................................................\n",
      ".................................................................................................\n",
      "................................................................................................\n",
      "...............................................................................................\n",
      "..............................................................................................\n",
      ".............................................................................................\n",
      "............................................................................................\n",
      "...........................................................................................\n",
      "..........................................................................................\n",
      ".........................................................................................\n",
      "........................................................................................\n",
      ".......................................................................................\n",
      "......................................................................................\n",
      ".....................................................................................\n",
      "....................................................................................\n",
      "...................................................................................\n",
      "..................................................................................\n",
      ".................................................................................\n",
      "................................................................................\n",
      "...............................................................................\n",
      "..............................................................................\n",
      ".............................................................................\n",
      "............................................................................\n",
      "...........................................................................\n",
      "..........................................................................\n",
      ".........................................................................\n",
      "........................................................................\n",
      ".......................................................................\n",
      "......................................................................\n",
      ".....................................................................\n",
      "....................................................................\n",
      "...................................................................\n",
      "..................................................................\n",
      ".................................................................\n",
      "................................................................\n",
      "...............................................................\n",
      "..............................................................\n",
      ".............................................................\n",
      "............................................................\n",
      "...........................................................\n",
      "..........................................................\n",
      ".........................................................\n",
      "........................................................\n",
      ".......................................................\n",
      "......................................................\n",
      ".....................................................\n",
      "....................................................\n",
      "...................................................\n",
      "..................................................\n",
      ".................................................\n",
      "................................................\n",
      "...............................................\n",
      "..............................................\n",
      ".............................................\n",
      "............................................\n",
      "...........................................\n",
      "..........................................\n",
      ".........................................\n",
      "........................................\n",
      ".......................................\n",
      "......................................\n",
      ".....................................\n",
      "....................................\n",
      "...................................\n",
      "..................................\n",
      ".................................\n",
      "................................\n",
      "...............................\n",
      "..............................\n",
      ".............................\n",
      "............................\n",
      "...........................\n",
      "..........................\n",
      ".........................\n",
      "........................\n",
      ".......................\n",
      "......................\n",
      ".....................\n",
      "....................\n",
      "...................\n",
      "..................\n",
      ".................\n",
      "................\n",
      "...............\n",
      "..............\n",
      ".............\n",
      "............\n",
      "...........\n",
      "..........\n",
      ".........\n",
      "........\n",
      ".......\n",
      "......\n",
      ".....\n",
      "....\n",
      "...\n",
      "..\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(\"Number of sentences = {:d}\".format(len(sentences))) \n",
    "\n",
    "dMatrix=[]\n",
    "for i in range(0,len(sentences)):\n",
    "    row=[]\n",
    "    dMatrix.append(row)\n",
    "    sys.stdout.write('\\n')\n",
    "    for j in range(0,len(sentences)):\n",
    "        if(i<j):\n",
    "            d = model.wmdistance(sentences[i], sentences[j])\n",
    "            row.append(d)\n",
    "            sys.stdout.write('.')\n",
    "            #print(\"(s_{:d},s_{:d}) = {:.2f}\".format(i,j,d))  \n",
    "        elif(i==j):\n",
    "            row.append(0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(sentences)):\n",
    "    for j in range(0,len(sentences)):\n",
    "        if(i>j):\n",
    "            d = dMatrix[j][i]\n",
    "            dMatrix[i].append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inlinks = []\n",
    "outlinks = []\n",
    "j=0\n",
    "for i, row in tsv.iterrows():\n",
    "    sid = row['SentenceId']\n",
    "    codeStr = row['Codes']\n",
    "    paragraph = row['Paragraph']\n",
    "    text = row['Sentence Text']\n",
    "    heading = row['Headings']\n",
    "    floatingBox = row['FloatingBox?']\n",
    "    discourse = row['Discourse Type']\n",
    "    reachData = row['friesEventsTypes']\n",
    "    \n",
    "    if(reachData == reachData):\n",
    "        allHits += 1\n",
    "\n",
    "    if (heading != heading):\n",
    "        heading = \"\"\n",
    "\n",
    "    if (floatingBox):\n",
    "        continue\n",
    "\n",
    "    if (('implication' not in discourse) and\n",
    "            'result' not in discourse):\n",
    "        continue\n",
    "\n",
    "    if ('methods' in heading.lower()):\n",
    "        continue\n",
    "    \n",
    "    if 'exLink' in codeStr:\n",
    "        outlinks.append(j)\n",
    "    else: \n",
    "        inlinks.append(j)\n",
    "    j += 1\n",
    "    \n",
    "ii = []\n",
    "io = []\n",
    "oo = []\n",
    "for i in range(0,len(sentences)):\n",
    "    for j in range(0,len(sentences)):\n",
    "        if( i in inlinks and j in inlinks):\n",
    "            ii.append(dMatrix[i][j])\n",
    "        elif( i in outlinks and j in outlinks):\n",
    "            oo.append(dMatrix[i][j])\n",
    "        else: \n",
    "            io.append(dMatrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8925,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(dMatrix)\n",
    "a_ii = np.array(iMatrix)\n",
    "a_i = np.array(inlinks)\n",
    "a_o = np.array(outlinks)\n",
    "\n",
    "print len(inlinks)\n",
    "\n",
    "a[a_ii].shape\n",
    "\n",
    "\n",
    "#print(\"mean = {:.2f}, stdev = {:.2f}\".format(np.mean(a),np.std(a)))\n",
    "#print inlinks\n",
    "#print outlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          2.70951278  3.28758438  2.96953717  3.17719459  2.81105121\n",
      "  3.35825087  3.46992865  2.29968692  3.29470469  3.05059795  2.63823061\n",
      "  2.98713756  2.84195812  3.40902733  2.99265033  2.90544037  2.73017843\n",
      "  3.22730077  3.28885195  2.88110962  2.56501906  2.92114793  2.54567875\n",
      "  2.7520886   3.26020055  2.69512023  2.69949859  7.02568466  3.0830886\n",
      "  2.44424402  2.8323922   3.29172478  2.11977776  2.6251516   3.31446568\n",
      "  2.94809315  3.23356727  2.99033033  2.99645548  3.24785576  2.50086568\n",
      "  3.34242533  3.31578968  3.32333538  2.51567323  3.1679571   2.77440658\n",
      "  3.24562005  2.56683298  2.54401009  2.19415479  2.24739697  3.38171277\n",
      "  2.82798443  2.85669571  3.11864001  2.65150638  3.22795487  2.24072686\n",
      "  2.75729774  3.07123952  2.79272931  2.66202148  2.08089359  2.70754171\n",
      "  2.34900525  2.88431247  2.43004056  3.19577731  2.67694075  2.29094977\n",
      "  7.33531289  2.95541549  2.73295739  3.05693131  2.9249465   3.35044991\n",
      "  2.68794189  2.74050119  2.66568818  2.89679137  2.74537219  7.22883396\n",
      "  2.79979104]\n"
     ]
    }
   ],
   "source": [
    "#y = np.arange(35).reshape(5,7)\n",
    "print a[a_i,a_i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  2.60492932,  2.91834837, ...,  3.29527311,\n",
       "         3.70035129,  3.84822942],\n",
       "       [ 0.        ,  3.14456507,  2.96953322, ...,  3.7744404 ,\n",
       "         3.99725877,  2.60492932],\n",
       "       [ 0.        ,  2.69037084,  2.34188279, ...,  3.55568761,\n",
       "         2.91834837,  2.96953322],\n",
       "       ..., \n",
       "       [ 0.        ,  3.67773797,  3.76203455, ...,  2.86753294,\n",
       "         3.14469046,  2.65721913],\n",
       "       [ 0.        ,  3.08264229,  3.70035129, ...,  3.4039933 ,\n",
       "         3.14241355,  3.14469046],\n",
       "       [ 0.        ,  3.84822942,  2.60492932, ...,  3.13584119,\n",
       "         2.65721913,  3.14469046]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = np.array(dMatrix)\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "Xlayout = model.fit_transform(X) \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(i),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    '''\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(digits.data.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox) \n",
    "    '''\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2cab381d1f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plot_embedding(Xlayout,\"t-SNE embedding of the sentences\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print sentences\n",
    "\n",
    "#plot_embedding(Xlayout,\"t-SNE embedding of the sentences\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing the demo above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features:', u'addresses, chicago, illinois, media, obama, president, press, speaks')\n"
     ]
    }
   ],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two documents are completely orthogonal in terms of bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 1, 1, 1, 0, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0]))\n",
      "cosine(doc_1, doc_2) = 1.00\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "v_1, v_2 = vect.transform([d1, d2])\n",
    "v_1 = v_1.toarray().ravel()\n",
    "v_2 = v_2.toarray().ravel()\n",
    "print(v_1, v_2)\n",
    "print(\"cosine(doc_1, doc_2) = {:.2f}\".format(cosine(v_1, v_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664012231\n",
      "d(addresses, speaks) = 0.33\n",
      "d(addresses, chicago) = 0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "#W_ = W[[vocab_dict[w] for w in vect.get_feature_names()]]\n",
    "#D_ = euclidean_distances(W_)\n",
    "print(\"d(addresses, speaks) = {:.2f}\".format(wv.similarity('addresses','speaks')))\n",
    "print(\"d(addresses, chicago) = {:.2f}\".format(wv.similarity('addresses','chicago')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [``pyemd``](https://github.com/wmayner/pyemd), a Python wrapper for [Pele and Werman's implementation of the earth mover's distance](http://www.ariel.ac.il/sites/ofirpele/fastemd/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-42dc2f4f34f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mv_1\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mv_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mv_2\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mv_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mD_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mD_\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mD_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# just for comparison purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d(doc_1, doc_2) = {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'D_' is not defined"
     ]
    }
   ],
   "source": [
    "from pyemd import emd\n",
    "\n",
    "# pyemd needs double precision input\n",
    "v_1 = v_1.astype(np.double)\n",
    "v_2 = v_2.astype(np.double)\n",
    "v_1 /= v_1.sum()\n",
    "v_2 /= v_2.sum()\n",
    "D_ = D_.astype(np.double)\n",
    "D_ /= D_.max()  # just for comparison purposes\n",
    "print(\"d(doc_1, doc_2) = {:.2f}\".format(emd(v_1, v_2, D_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification\n",
    "\n",
    "We will use the [*20 Newsgroups*](http://qwone.com/~jason/20Newsgroups/) classification task.  Because WMD is an expensive computation, for this demo we just use a subset.  To emphasize the power of the method, we use a larger test size, but train on relatively few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups()\n",
    "docs, y = newsgroups.data, newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y,\n",
    "                                                          train_size=100,\n",
    "                                                          test_size=300,\n",
    "                                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `W` embedding array is pretty huge, we might as well restrict it to just the words that actually occur in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(docs_train + docs_test)\n",
    "common = [word for word in vect.get_feature_names() if word in vocab_dict]\n",
    "W_common = W[[vocab_dict[w] for w in common]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a fixed-vocabulary vectorizer using only the words we have embeddings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(vocabulary=common, dtype=np.double)\n",
    "X_train = vect.fit_transform(docs_train)\n",
    "X_test = vect.transform(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to proceed is to just pre-compute the pairwise distances between all documents, and use them to search for hyperparameters and evaluate the model. However, that would incur some extra computation, and WMD is expensive. Also, it's not the most pleasant user interface. So we define some scikit-learn compatible estimators for computing the WMD.\n",
    "\n",
    "**`WordMoversKNN`** subclasses from `KNeighborsClassifier` and overrides the `predict` function to compute the WMD between all training and test samples.\n",
    "\n",
    "In practice, however, we often don't know what is the best `n_neighbors` to use.  Simply wrapping `WordMoversKNN` in a `GridSearchCV` would be rather expensive because of all the distances that would need to be recomputed for every value of `n_neighbors`. So we introduce **`WordMoversKNNCV`**, which, when fitted, performs *cross-validation* to find the best value of `n_neighbors` (under any given evaluation metric), while only computing the WMD once per fold, and only across folds (saving `n_folds * fold_size ** 2` evaluations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_movers_knn.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"%%file word_movers_knn.py\"\"\"\n",
    "\n",
    "# Authors: Vlad Niculae, Matt Kusner\n",
    "# License: Simplified BSD\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.cross_validation import check_cv\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from pyemd import emd\n",
    "\n",
    "\n",
    "class WordMoversKNN(KNeighborsClassifier):\n",
    "    \"\"\"K nearest neighbors classifier using the Word Mover's Distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    W_embed : array, shape: (vocab_size, embed_size)\n",
    "        Precomputed word embeddings between vocabulary items.\n",
    "        Row indices should correspond to the columns in the bag-of-words input.\n",
    "\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for :meth:`k_neighbors` queries.\n",
    "\n",
    "    n_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for Word Mover's Distance computation.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "    \n",
    "    verbose : int, optional\n",
    "        Controls the verbosity; the higher, the more messages. Defaults to 0.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger\n",
    "    From Word Embeddings To Document Distances\n",
    "    The International Conference on Machine Learning (ICML), 2015\n",
    "    http://mkusner.github.io/publications/WMD.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    _pairwise = False\n",
    "\n",
    "    def __init__(self, W_embed, n_neighbors=1, n_jobs=1, verbose=False):\n",
    "        self.W_embed = W_embed\n",
    "        self.verbose = verbose\n",
    "        super(WordMoversKNN, self).__init__(n_neighbors=n_neighbors, n_jobs=n_jobs,\n",
    "                                            metric='precomputed', algorithm='brute')\n",
    "\n",
    "    def _wmd(self, i, row, X_train):\n",
    "        \"\"\"Compute the WMD between training sample i and given test row.\n",
    "        \n",
    "        Assumes that `row` and train samples are sparse BOW vectors summing to 1.\n",
    "        \"\"\"\n",
    "        union_idx = np.union1d(X_train[i].indices, row.indices)\n",
    "        W_minimal = self.W_embed[union_idx]\n",
    "        W_dist = euclidean_distances(W_minimal)\n",
    "        bow_i = X_train[i, union_idx].A.ravel()\n",
    "        bow_j = row[:, union_idx].A.ravel()\n",
    "        return emd(bow_i, bow_j, W_dist)\n",
    "    \n",
    "    def _wmd_row(self, row, X_train):\n",
    "        \"\"\"Wrapper to compute the WMD of a row with all training samples.\n",
    "        \n",
    "        Assumes that `row` and train samples are sparse BOW vectors summing to 1.\n",
    "        Useful for parallelization.\n",
    "        \"\"\"\n",
    "        n_samples_train = X_train.shape[0]\n",
    "        return [self._wmd(i, row, X_train) for i in range(n_samples_train)]\n",
    "\n",
    "    def _pairwise_wmd(self, X_test, X_train=None):\n",
    "        \"\"\"Computes the word mover's distance between all train and test points.\n",
    "        \n",
    "        Parallelized over rows of X_test.\n",
    "        \n",
    "        Assumes that train and test samples are sparse BOW vectors summing to 1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test: scipy.sparse matrix, shape: (n_test_samples, vocab_size)\n",
    "            Test samples.\n",
    "        \n",
    "        X_train: scipy.sparse matrix, shape: (n_train_samples, vocab_size)\n",
    "            Training samples. If `None`, uses the samples the estimator was fit with.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dist : array, shape: (n_test_samples, n_train_samples)\n",
    "            Distances between all test samples and all train samples.\n",
    "        \n",
    "        \"\"\"\n",
    "        n_samples_test = X_test.shape[0]\n",
    "        \n",
    "        if X_train is None:\n",
    "            X_train = self._fit_X\n",
    "\n",
    "        dist = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n",
    "            delayed(self._wmd_row)(test_sample, X_train)\n",
    "            for test_sample in X_test)\n",
    "\n",
    "        return np.array(dist)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using X as training data and y as target values\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse matrix, shape: (n_samples, n_features)\n",
    "            Training data. \n",
    "\n",
    "        y : {array-like, sparse matrix}\n",
    "            Target values of shape = [n_samples] or [n_samples, n_outputs]\n",
    "\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "        return super(WordMoversKNN, self).fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the provided data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy.sparse matrix, shape (n_test_samples, vocab_size)\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape [n_samples]\n",
    "            Class labels for each data sample.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "        dist = self._pairwise_wmd(X)\n",
    "        return super(WordMoversKNN, self).predict(dist)\n",
    "    \n",
    "    \n",
    "class WordMoversKNNCV(WordMoversKNN):\n",
    "    \"\"\"Cross-validated KNN classifier using the Word Mover's Distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W_embed : array, shape: (vocab_size, embed_size)\n",
    "        Precomputed word embeddings between vocabulary items.\n",
    "        Row indices should correspond to the columns in the bag-of-words input.\n",
    "\n",
    "    n_neighbors_try : sequence, optional\n",
    "        List of ``n_neighbors`` values to try.\n",
    "        If None, tries 1-5 neighbors.\n",
    "\n",
    "    scoring : string, callable or None, optional, default: None\n",
    "        A string (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "        For integer/None inputs, StratifiedKFold is used.\n",
    "\n",
    "    n_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for Word Mover's Distance computation.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "\n",
    "    verbose : int, optional\n",
    "        Controls the verbosity; the higher, the more messages. Defaults to 0.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cv_scores_ : array, shape (n_folds, len(n_neighbors_try))\n",
    "        Test set scores for each fold.\n",
    "\n",
    "    n_neighbors_ : int,\n",
    "        The best `n_neighbors` value found.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger\n",
    "    From Word Embeddings To Document Distances\n",
    "    The International Conference on Machine Learning (ICML), 2015\n",
    "    http://mkusner.github.io/publications/WMD.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, W_embed, n_neighbors_try=None, scoring=None, cv=3,\n",
    "                 n_jobs=1, verbose=False):\n",
    "        self.cv = cv\n",
    "        self.n_neighbors_try = n_neighbors_try\n",
    "        self.scoring = scoring\n",
    "        super(WordMoversKNNCV, self).__init__(W_embed,\n",
    "                                              n_neighbors=None,\n",
    "                                              n_jobs=n_jobs,\n",
    "                                              verbose=verbose)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit KNN model by choosing the best `n_neighbors`.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X : scipy.sparse matrix, (n_samples, vocab_size)\n",
    "            Data\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target\n",
    "        \"\"\"\n",
    "        if self.n_neighbors_try is None:\n",
    "            n_neighbors_try = range(1, 6)\n",
    "        else:\n",
    "            n_neighbors_try = self.n_neighbors_try\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', copy=True)\n",
    "        X = normalize(X, norm='l1', copy=False)\n",
    "\n",
    "        cv = check_cv(self.cv, X, y)\n",
    "        knn = KNeighborsClassifier(metric='precomputed', algorithm='brute')\n",
    "        scorer = check_scoring(knn, scoring=self.scoring)\n",
    "\n",
    "        scores = []\n",
    "        for train_ix, test_ix in cv:\n",
    "            dist = self._pairwise_wmd(X[test_ix], X[train_ix])\n",
    "            knn.fit(X[train_ix], y[train_ix])\n",
    "            scores.append([\n",
    "                scorer(knn.set_params(n_neighbors=k), dist, y[test_ix])\n",
    "                for k in n_neighbors_try\n",
    "            ])\n",
    "        scores = np.array(scores)\n",
    "        self.cv_scores_ = scores\n",
    "\n",
    "        best_k_ix = np.argmax(np.mean(scores, axis=0))\n",
    "        best_k = n_neighbors_try[best_k_ix]\n",
    "        self.n_neighbors = self.n_neighbors_ = best_k\n",
    "\n",
    "        return super(WordMoversKNNCV, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=3)]: Done  34 out of  34 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordMoversKNNCV(W_embed=memmap([[ 0.04283, -0.01124, ..., -0.05679, -0.00763],\n",
       "       [ 0.02884, -0.05923, ..., -0.04744,  0.06698],\n",
       "       ...,\n",
       "       [ 0.08428, -0.15534, ..., -0.01413,  0.04561],\n",
       "       [-0.02052,  0.08666, ...,  0.03659,  0.10445]]),\n",
       "        cv=3, n_jobs=3, n_neighbors_try=range(1, 20), scoring=None,\n",
       "        verbose=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv = WordMoversKNNCV(cv=3,\n",
    "                         n_neighbors_try=range(1, 20),\n",
    "                         W_embed=W_common, verbose=5, n_jobs=3)\n",
    "knn_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.38\n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:.2f}\".format(knn_cv.cv_scores_.mean(axis=0).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed: 48.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.2f}\".format(knn_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with other models\n",
    "\n",
    "Now let's see how WMD compares with some common approaches, on bag of words features.  The most apples-to-apples comparison would be\n",
    "K nearest neighbors with a cosine similarity metric. This approach performs worse than using WMD. (All scores are accuracies.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.34\n",
      "Test score: 0.22\n"
     ]
    }
   ],
   "source": [
    "knn_grid = GridSearchCV(KNeighborsClassifier(metric='cosine', algorithm='brute'),\n",
    "                        dict(n_neighbors=list(range(1, 20))),\n",
    "                        cv=3)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "print(\"CV score: {:.2f}\".format(knn_grid.best_score_))\n",
    "print(\"Test score: {:.2f}\".format(knn_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common method for text classification is the linear support vector machine on bag of words.\n",
    "This performs a bit better than vanilla cosine KNN, but worse than using WMD in this setting.  In our experience,\n",
    "this seems to depend on the amount of training data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.35\n",
      "Test score: 0.27\n"
     ]
    }
   ],
   "source": [
    "svc_grid = GridSearchCV(LinearSVC(),\n",
    "                        dict(C=np.logspace(-6, 6, 13, base=2)),\n",
    "                        cv=3)\n",
    "svc_grid.fit(X_train, y_train)\n",
    "print(\"CV score: {:.2f}\".format(svc_grid.best_score_))\n",
    "print(\"Test score: {:.2f}\".format(svc_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learned?\n",
    "\n",
    "WMD is much better at capturing semantic similarity between documents than cosine, due to its ability to generalize to unseen words.  The SVM does somewhat better than cosine KNN, but still lacks such out-of-vocabulary generalization.   Given enough data, WMD can probably improve this margin, especially using something like metric learning on top.\n",
    "\n",
    "The exact WMD, as we have used it here, is pretty slow.  This code is not optimized as much as it could be, there is potential through caching and using Cython.\n",
    "However, a major limitation remains the cost of actually computing the EMD. To scale even higher, exactness can be relaxed by using lower bounds. In our next post, we will compare such optimization strategies, as discussed in [the WMD paper](http://mkusner.github.io/publications/WMD.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
